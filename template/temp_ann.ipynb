{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i. Perkenalan\n",
    "\n",
    "Nama : Akbar Fitriawan  \n",
    "Batch : HCK-15\n",
    "\n",
    "Tentang dataset:\n",
    "\n",
    "\n",
    "Objective:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ii. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library dataframe\n",
    "import pandas as pd\n",
    "# library olah data numerical\n",
    "import numpy as np\n",
    "# library statistik\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "\n",
    "# Data visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "# libraries Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split # splittingdata\n",
    "from feature_engine.outliers import Winsorizer # outlier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler # scalling\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder # encoder\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Library handling outlier\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "# Library to suppress warnings or deprecation notes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jumlah Baris: ',df.shape[0])\n",
    "print('Jumlah Kolom: ',df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 5 teratas \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 5 Terbawah\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight:\n",
    "- Dataset memiliki `1470` jumlah baris dan `22` jumlah kolom\n",
    "- Data type int64(13), object(9)\n",
    "- Tidak ada indikasi missing values\n",
    "- EmployeeID tidak digunakan karena tidak ada informasi yang dapat diambil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Data Info \n",
    "df.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iv. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataset\n",
    "data_analyz = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Data attration employee\n",
    "plt.figure(figsize=(8,6))\n",
    "data_analyz.Attrition.value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Total Count Attrion Employee')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump dataset agar tidak mengulang\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek duplicated data\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribution selected feature\n",
    "to_Plot = ['Age', 'DistanceFromHome', 'MonthlyIncome',\n",
    "       'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "       'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole',\n",
    "       'YearsSinceLastPromotion', 'YearsWithCurrentManager','Attrition']\n",
    "plt.figure()\n",
    "sns.pairplot(data[to_Plot],hue='Attrition')  \n",
    "#Taking hue \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data feature dan target\n",
    "X = data.drop('Attrition', axis=1)\n",
    "y = data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting between Train-Set, Val-Set, and Test-Set\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Train Size : ', X_train.shape)\n",
    "print('Val Size   : ', X_val.shape)\n",
    "print('Test Size  : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tampilkan_nilai_unik(data):\n",
    "    for column in data.columns:\n",
    "        print(f\"{column}: {data[column].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tampilkan_nilai_unik(X_train[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique().to_frame().reset_index().rename(columns={'index':'Column',0:'Unique_values'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing Values on X_train\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing Values on X_val\n",
    "\n",
    "X_val.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing Values on X_test\n",
    "\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outlier\n",
    "\n",
    "# plot figure \n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "# membuat boxplot\n",
    "for i, column in enumerate(num):\n",
    "    axes[i].boxplot(data[column])\n",
    "    axes[i].set_title(column)\n",
    "    \n",
    "# hapus plot kosong\n",
    "for j in range(len(num), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil boxplot yang memiliki outlier adalah `MonthlyIncome`, `TotalWorkingYears`, `YearsAtCompany`, `YearsCurrentRole`,`YearsSinceLastPromotion`, dan `YearsWithCurrentManager`. adapun kolom lain yang memiliki outlier tapi bukankah fakta nyata jadi saya abaikan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti penjelasan di atas adanya outlier sehingga harus di atasi, dalam hal ini saya menggunakan metode Winsorization.\n",
    "\n",
    "Cara melakukannya adalah memfilter ditribusi normal dan tidak dengan menggunakan motode statistika skewnes dan menset threshold.\n",
    "- Threshold untuk Distribusi Normal: \n",
    "Biasanya, nilai skewness yang mendekati nol (`misalnya, antara -0.5 hingga 0.5`) menunjukkan bahwa distribusi data cenderung normal. \n",
    "- Threshold untuk Distribusi Skewed:\n",
    "Distribusi data dikatakan skewed jika nilai skewnessnya melebihi nilai ambang tertentu. Secara umum, `nilai skewness di atas 0.5 atau di bawah -0.5` sering digunakan sebagai indikator bahwa distribusi data tidak simetris dan cenderung skewed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Numerical Columns into Variable\n",
    "# num_cols = X_train.select_dtypes(include=np.number).columns.to_list()\n",
    "num_cols = ['Age', 'DistanceFromHome','MonthlyIncome','PercentSalaryHike','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrentManager']\n",
    "\n",
    "# Making data and columns for normal distribution\n",
    "dataNum_normal = []\n",
    "listCols_normal = []\n",
    "\n",
    "# Making data and columns for skewed distribution\n",
    "dataNum_skewed = []\n",
    "listCols_skewed = []\n",
    "\n",
    "# For loop in every numerical column to filer the data distribution into either normal distributed or skewed columns\n",
    "for cols in num_cols:\n",
    "    skewness = X_train[cols].skew()\n",
    "    \n",
    "    # If the data normally distributed\n",
    "    if skewness <= 0.5 and skewness >= -0.5:            \n",
    "        listCols_normal.append(cols)\n",
    "        dataNum_normal.append([cols, skewness])\n",
    "        \n",
    "    # Elif the data is skewed\n",
    "    elif skewness < -0.5 or skewness > 0.5:\n",
    "        listCols_skewed.append(cols)\n",
    "        dataNum_skewed.append([cols, skewness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing skewed columns\n",
    "data_normal = pd.DataFrame(data=dataNum_normal, columns=['kolom_normal', 'skewness'])\n",
    "data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing skewed columns\n",
    "data_skew = pd.DataFrame(data=dataNum_skewed, columns=['kolom_skewed', 'skewness'])\n",
    "data_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input low variation columns into variable\n",
    "# col_lowVarition = ['PerformanceRating']\n",
    "\n",
    "# # Drop the low variation columns in skewed columns, and add it into normal distribution columns\n",
    "# for kolom in col_lowVarition:\n",
    "#     listCols_skewed.remove(kolom) #-> removing low variation columns in skewed columns\n",
    "#     listCols_normal.append(kolom) #-> appending low variation columns in normal distributed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping Method for Normal Distribution  \n",
    "winsorizer_normal_dist = Winsorizer(capping_method='gaussian',\n",
    "                            tail='both',\n",
    "                            fold=3,\n",
    "                            variables=listCols_normal,\n",
    "                            missing_values='ignore')\n",
    "\n",
    "# Fit & Transforming X_train \n",
    "X_train_capped = winsorizer_normal_dist.fit_transform(X_train)\n",
    "\n",
    "# Transforming X_test\n",
    "X_val_capped = winsorizer_normal_dist.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorizer_skewed = Winsorizer(capping_method='iqr',\n",
    "                            tail='both',\n",
    "                            fold=1.5,\n",
    "                            variables=listCols_skewed,\n",
    "                            missing_values='ignore')\n",
    "\n",
    "# Fit & Transforming X_train \n",
    "X_train_capped = winsorizer_skewed.fit_transform(X_train_capped)\n",
    "\n",
    "# Transforming X_test\n",
    "X_val_capped = winsorizer_skewed.transform(X_val_capped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution Comparison\n",
    "def outlier_handling_plot_comparison(df_before, df_after, variable):\n",
    "    \"\"\"\n",
    "    This function is created to plot histograms and boxplots for a variable before and after outlier handling\n",
    "    \n",
    "    Parameters:\n",
    "    - df_before (pandas.DataFrame): DataFrame before outlier handling\n",
    "    - df_after (pandas.DataFrame): DataFrame after outlier handling\n",
    "    - variable (str): The variable to plot\n",
    "    \n",
    "    Example: \n",
    "    num_cols = ['numeric_column']                                   <- Enter the numeric column\n",
    "    for col in num_cols:                                            <- make a for loop in numeric column to access every columns\n",
    "        plot_distribution_comparison(X_train, X_train_capped, col)  <- put X_train, X_train_capped, and columns in the variable\n",
    "\n",
    "    \"\"\"\n",
    "    # Figure Size, and Super Title based on variable\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))                               \n",
    "    fig.suptitle(f'{variable} - Distribution Before and After Outlier Handling')\n",
    "\n",
    "    # Plot Histogram Before\n",
    "    sns.histplot(df_before[variable], bins=30, ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Histogram Before')\n",
    "\n",
    "    # Plot Boxplot Before\n",
    "    sns.boxplot(y=df_before[variable], ax=axes[1, 0], color='lightgreen')\n",
    "    axes[1, 0].set_title('Boxplot Before')\n",
    "\n",
    "    # Plot Histogram After\n",
    "    sns.histplot(df_after[variable], bins=30, ax=axes[0, 1], color='skyblue')\n",
    "    axes[0, 1].set_title('Histogram After')\n",
    "\n",
    "    # Plot Boxplot After\n",
    "    sns.boxplot(y=df_after[variable], ax=axes[1, 1], color='lightgreen')\n",
    "    axes[1, 1].set_title('Boxplot After')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Function\n",
    "for col in num_cols:\n",
    "    outlier_handling_plot_comparison(X_train, X_train_capped, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data Numerik dan Kategorik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column numeric list\n",
    "# num_columns = X_train_capped.select_dtypes(include=np.number).columns.tolist()\n",
    "num_columns = [\n",
    "    'Age',\n",
    "    'DistanceFromHome',\n",
    "    'MonthlyIncome',\n",
    "    'NumCompaniesWorked',\n",
    "    'PercentSalaryHike',\n",
    "    'PerformanceRating',\n",
    "    'TotalWorkingYears',\n",
    "    'YearsAtCompany',\n",
    "    'YearsInCurrentRole',\n",
    "    'YearsSinceLastPromotion',\n",
    "    'YearsWithCurrentManager'\n",
    "]\n",
    "# column categorical list\n",
    "# cat_columns = X_train_capped.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_columns = [\n",
    "    'BusinessTravel',\n",
    "    'Department',\n",
    "    'Education',\n",
    "    'EducationField',\n",
    "    'Gender',\n",
    "    'JobRole',\n",
    "    'JobSatisfaction',\n",
    "    'MaritalStatus',\n",
    "    'WorkLifeBalance'\n",
    "]\n",
    "\n",
    "\n",
    "print('Numerical List: ', num_columns)\n",
    "print('Catgorical List: ', cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override X_train with spliting list\n",
    "X_train_num = X_train_capped[num_columns]\n",
    "X_train_cat = X_train_capped[cat_columns]\n",
    "# override X_test with spliting list\n",
    "X_val_num = X_val_capped[num_columns]\n",
    "X_val_cat = X_val_capped[cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size X_train_num:',X_train_num.shape)\n",
    "print('Size X_train_cat:',X_train_cat.shape)\n",
    "print('\\n')\n",
    "print('Size X_test_num:',X_val_num.shape)\n",
    "print('Size X_test_cat:',X_val_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam melakukan Feature Selection yaitu dengan membagi kolom Numerical dan Categorical, Kemudian mencari korelasi yang berpengaruh terhadap target(label) dengan cara metode statitika. \n",
    "\n",
    "Mencari Korelasi:\n",
    "- Untuk Categorical biasanya menggunakan `Kendall Tau's` dan Numerical menggunakan `pearsonr`(distribusi normal) serta `spearmanr` (distribusi skewed/not gaussian)\n",
    "- Set treshold P-value 0,05.\n",
    "    - jika nilai p-value kurang dari `< 0.05`, yang berarti terdapat korelasi yang signifikan antara variabel yang dianalisis\n",
    "    - Jika nilai p-value sama dengan atau lebih besar dari `>= 0.05 atau > 0.05`, yang berarti tidak terdapat korelasi yang signifikan antara variabel yang dianalisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between categorical columns and 'Attrition' using Kendall Tau's correlation\n",
    "\n",
    "p_values = []\n",
    "interpretation = []\n",
    "cols = []\n",
    "corr = []\n",
    "\n",
    "\n",
    "for col in X_train_cat.columns:\n",
    "  corr_coef, p_value = kendalltau(X_train_cat[col], y_train)\n",
    "\n",
    "  p_values.append(p_value)\n",
    "  cols.append(col)\n",
    "  corr.append(corr_coef)\n",
    "\n",
    "  if p_value < 0.05:\n",
    "    interpretation.append('Significant Correlation')\n",
    "  else :\n",
    "    interpretation.append('No Significant Correlation')\n",
    "\n",
    "cat_corr= pd.DataFrame({'Column Name':cols,\n",
    "              'Correlation Coefficient' : corr,\n",
    "              'P-value':p_values,\n",
    "              'Interpretation': interpretation })\n",
    "\n",
    "cat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter corelation categorical\n",
    "cat_corr = cat_corr[cat_corr['Interpretation'] == \"Significant Correlation\"]\n",
    "\n",
    "cat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat = cat_corr['Column Name'].values.tolist()\n",
    "list_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jadi feature yang di pakai dari categorical column yaitu ('Department', 'JobRole', 'MaritalStatus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between numerical columns and 'price' using pearsonr and spearmanr correlation\n",
    "\n",
    "p_values = []\n",
    "interpretation = []\n",
    "cols = []\n",
    "corr = []\n",
    "\n",
    "for col in X_train_num.columns:\n",
    "  if abs(X_train_num[col].skew()) < 0.5:                    #For Normally Distributed Columns\n",
    "    corr_coef, p_value = pearsonr(X_train_num[col], y_train)\n",
    "\n",
    "    p_values.append(p_value)\n",
    "    cols.append(col)\n",
    "    corr.append(corr_coef)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "      interpretation.append('Significant Correlation')\n",
    "    else :\n",
    "      interpretation.append('No Significant Correlation')\n",
    "  else:                                                     #For Non Normally Distributed Columns\n",
    "    corr_coef, p_value = spearmanr(X_train_num[col], y_train)\n",
    "\n",
    "    p_values.append(p_value)\n",
    "    cols.append(col)\n",
    "    corr.append(corr_coef)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "      interpretation.append('Significant Correlation')\n",
    "    else :\n",
    "      interpretation.append('No Significant Correlation')\n",
    "\n",
    "num_corr = pd.DataFrame({'Column Name':cols,\n",
    "              'Correlation Coefficient' : corr,\n",
    "              'P-value':p_values,\n",
    "              'Interpretation': interpretation })\n",
    "\n",
    "num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter corelation categorical\n",
    "num_corr = num_corr[num_corr['Interpretation'] == \"Significant Correlation\"]\n",
    "num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num  = num_corr['Column Name'].values.tolist()\n",
    "list_num "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari korelasi Numerical column yaitu ('Age','MonthlyIncome','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole','YearsWithCurrentManager')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Membuat variable baru Untuk Feature yang berkolerasi -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print list correlation\n",
    "print('column katgorik:', list_cat)\n",
    "print('column numerik:', list_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append ke X feature yang berkorelasi\n",
    "# X_train_cat = X_train_cat[list_cat]\n",
    "# X_train_num = X_train_num[list_num]\n",
    "\n",
    "# X_val_cat = X_val_cat[list_cat]\n",
    "# X_val_num = X_val_num[list_num]\n",
    "\n",
    "# X_test_cat = X_test[list_cat]\n",
    "# X_test_num = X_test[list_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Size X_train_num:',X_train_num.shape)\n",
    "# print('Size X_train_cat:',X_train_cat.shape)\n",
    "# print('')\n",
    "# print('Size X_val_num:',X_val_num.shape)\n",
    "# print('Size X_val_cat:',X_val_cat.shape)\n",
    "# print('')\n",
    "# print('Size X_test_num:',X_test_num.shape)\n",
    "# print('Size X_test_cat:',X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the numerical data\n",
    "# X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the categorical data\n",
    "# X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the numerical data\n",
    "# X_val_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the categorical data\n",
    "# X_val_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the numerical data\n",
    "# X_test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the numerical data\n",
    "# X_test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat\n",
    "list_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                             StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(OneHotEncoder())\n",
    "\n",
    "final_pipeline = ColumnTransformer([\n",
    "    ('pipe_num', num_pipeline, list_num),\n",
    "    ('pipe_cat', cat_pipeline, list_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Transform\n",
    "\n",
    "X_train = final_pipeline.fit_transform(X_train)\n",
    "X_val = final_pipeline.transform(X_val)\n",
    "X_test = final_pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distribution selected feature\n",
    "plt.figure()\n",
    "sns.pairplot(X_test_final)  \n",
    "#Taking hue \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vi. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Model Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes of hyperparameter `loss` :\n",
    "* Use `sparse_categorical_crossentropy` :\n",
    "  + The target just a class index/number such as from `0` to `9` like in this case\n",
    "\n",
    "* Use `categorical_crossentropy` :\n",
    "  + The target is in form of one-hot vectors\n",
    "  + To represent class `3` the target will be `[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.a Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Session\n",
    "seed = 20\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "ann = Sequential()\n",
    "# ann.add() #input\n",
    "ann.add(Dense(300, activation='relu', input_shape=(X_train.shape[1:]), kernel_initializer=tf.keras.initializers.HeNormal(seed))) #hidden\n",
    "ann.add(Dense(100, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed))) #hidden\n",
    "ann.add(Dense(300, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed))) #hidden\n",
    "ann.add(Dense(100, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed))) #hidden\n",
    "ann.add(Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.GlorotNormal(seed))) #output\n",
    "\n",
    "# Compile\n",
    "ann.compile(loss='binary_crossentropy', #sparse_categorical_crossentropy\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Layers\n",
    "\n",
    "tf.keras.utils.plot_model(model_sequential_mnist, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.a Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1= ann.fit(X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.a Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Results\n",
    "\n",
    "history_1_df = pd.DataFrame(history_1.history)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=history_1_df[['loss', 'val_loss']])\n",
    "plt.grid()\n",
    "plt.title('Loss vs Val-Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=history_1_df[['accuracy', 'val_accuracy']])\n",
    "plt.grid()\n",
    "plt.title('Accuracy vs Val-Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Utilities\n",
    "\n",
    "# fungsi evaluasi\n",
    "def performance_check(clf, X, y):\n",
    "  y_pred = clf.predict(X)\n",
    "  # print('Accuracy Score - : ', accuracy_score(y, y_pred)) # metric Accuracy score\n",
    "  # print('Precision Score - : ', precision_score(y, y_pred)) # metric precision score\n",
    "  print('Recall Score - : ', recall_score(y, y_pred)) # metric recall score\n",
    "  # print('F1 Score - : ', f1_score(y, y_pred)) # metric f1 score\n",
    "  print('Jumlah class - : ' , Counter(y_pred),'\\n')\n",
    "  cm = confusion_matrix(y, y_pred)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "  disp.plot()\n",
    "  plt.show()\n",
    "  print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= ann.evaluate(X_train,y_train)\n",
    "print('loss',result[0])\n",
    "print('Acc',result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= ann.evaluate(X_val,y_val)\n",
    "print('loss',result[0])\n",
    "print('Acc',result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= ann.evaluate(X_test,y_test)\n",
    "print('loss',result[0])\n",
    "print('Acc',result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binnary\n",
    "y_test_proba= ann.predict(X_test)\n",
    "y_test_pred= tf.where(y_test_proba >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# Probability for each class\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=class_names)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# Probability for each class\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=class_names)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# Get class prediction\n",
    "\n",
    "y_pred_class = np.argmax(y_pred[0])\n",
    "y_pred_class_name = class_names[np.argmax(y_pred[0])]\n",
    "\n",
    "print('Prediction - Class       : ', y_pred_class)\n",
    "print('Prediction - Class Name  : ', y_pred_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# Check the performance of test-set\n",
    "\n",
    "## Get the probability\n",
    "y_pred_mnist_proba = model_sequential_mnist.predict(X_test)\n",
    "\n",
    "## Get class with maximum probability\n",
    "y_pred_mnist = np.argmax(y_pred_mnist_proba, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# Check the performance of test-set\n",
    "\n",
    "## Get the probability\n",
    "y_pred_mnist_proba = model_sequential_mnist.predict(X_test)\n",
    "\n",
    "## Get class with maximum probability\n",
    "y_pred_mnist = np.argmax(y_pred_mnist_proba, axis=-1)\n",
    "\n",
    "## Display Classification Report\n",
    "print(classification_report(y_test, y_pred_mnist, target_names=np.array(class_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Model Functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.b Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Session\n",
    "seed = 20\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#input layer\n",
    "input_layer= tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "#hidden layer di cabang pertama\n",
    "dense_1= Dense(300, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_1_1')(input_layer)\n",
    "dense_2= Dense(100, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_1_2')(dense_1)\n",
    "\n",
    "#hidden layer di cabang kedua\n",
    "dense_3= Dense(300, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_2_1')(input_layer)\n",
    "dense_4= Dense(100, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_2_2')(dense_3)\n",
    "\n",
    "# concat cabang pertama dan kedua\n",
    "layer_concat= tf.keras.layers.Concatenate()([dense_2,dense_4])\n",
    "\n",
    "#output layer\n",
    "output_layer= Dense(1, activation='sigmoid',kernel_initializer=tf.keras.initializers.GlorotNormal(seed))(layer_concat)\n",
    "\n",
    "#buat model\n",
    "model_func= Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile\n",
    "model_func.compile(loss='binary_crossentropy',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi class (img)\n",
    "# Clear Session\n",
    "seed = 20\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#input layer\n",
    "input_layer= tf.keras.layers.Input(shape=[28, 28])\n",
    "flatten_layer = Flatten()(input_layer)\n",
    "\n",
    "#hidden layer di cabang pertama\n",
    "dense_1= Dense(16384, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_1_1')(flatten_layer)\n",
    "\n",
    "dense_2= Dense(512, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_1_2')(dense_1)\n",
    "\n",
    "dense_3= Dense(784, activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.HeNormal(seed),\n",
    "               name='hidden_2_3')(dense_2)\n",
    "\n",
    "reshape_layer = tf.keras.layers.Reshape((28,28))(dense_3)\n",
    "\n",
    "\n",
    "# concat cabang pertama dan kedua\n",
    "layer_concat= tf.keras.layers.Concatenate()([input_layer,reshape_layer])\n",
    "\n",
    "flatten_layer2 = Flatten()(layer_concat)\n",
    "\n",
    "#output layer\n",
    "output_layer= Dense(10, activation='softmax')(flatten_layer2)\n",
    "\n",
    "#buat model\n",
    "model_func= Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile\n",
    "model_func.compile(loss='sparse_categorical_crossentropy',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Layers\n",
    "\n",
    "tf.keras.utils.plot_model(model_func, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Layers\n",
    "\n",
    "tf.keras.utils.plot_model(model_func, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.b Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2= model_func.fit(X_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi class \n",
    "# Train the Model\n",
    "\n",
    "%%time\n",
    "history_model_func = model_func.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.b Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba= model_func.predict(X_test)\n",
    "y_test_pred= tf.where(y_test_proba >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the preprocessing to a file\n",
    "with open('preprocessing.pkl', 'wb') as file:\n",
    "    pickle.dump(final_pipeline, file)\n",
    "\n",
    "# Load the preprocessing from the file\n",
    "with open('preprocessing.pkl', 'rb') as file:\n",
    "    preproc_final = pickle.load(file)\n",
    "\n",
    "#save model\n",
    "ann.save('model_ann.keras')\n",
    "\n",
    "# load model\n",
    "model_final= load_model('model_ann.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf= data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf.drop(['PassengerId', 'Cabin', 'Ticket', 'Embarked', 'Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf_preproc= preproc_final.transform(data_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba=model_final.predict(data_inf_preproc)\n",
    "pred= tf.where(pred_proba >=0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf['predict']= pred\n",
    "data_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vii. Kesimpulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
