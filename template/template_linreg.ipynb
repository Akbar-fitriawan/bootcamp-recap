{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i. Perkenalan\n",
    "\n",
    "Nama : Akbar Fitriawan  \n",
    "Batch : HCK-15\n",
    "\n",
    "Tentang dataset:\n",
    "\n",
    "\n",
    "Objective:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ii. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library dataframe\n",
    "import pandas as pd\n",
    "# library olah data numerical\n",
    "import numpy as np\n",
    "# library statistik\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "\n",
    "# Data visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# libraries model mechine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# libraries Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.outliers import Winsorizer # outlier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler # scalling\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder # encoder\n",
    "\n",
    "# Libraries model evaluation\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Display QQ plot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Libraries model saving\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "# Library to suppress warnings or deprecation notes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Jumlah Baris: ',df.shape[0])\n",
    "print('Jumlah Kolom: ',df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 5 teratas \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 5 Terbawah\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iv. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Membuat DataFrame contoh\n",
    "# data = {'nama': ['Andi', 'Budi', 'Caca', 'Deni'],\n",
    "#         'usia': [10, 20, 30, 60]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Fungsi untuk mengkategorikan usia\n",
    "# def kategorikan_usia(usia):\n",
    "#     if usia <= 12:\n",
    "#         return 'anak-anak'\n",
    "#     elif usia <= 20:\n",
    "#         return 'remaja'\n",
    "#     elif usia <= 50:\n",
    "#         return 'dewasa'\n",
    "#     else:\n",
    "#         return 'orang tua'\n",
    "\n",
    "# # Membuat kolom baru dengan hasil filter\n",
    "# df['kategori_usia'] = df['usia'].apply(kategorikan_usia)\n",
    "\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Membuat DataFrame contoh\n",
    "# data = {'nama': ['Andi', 'Budi', 'Caca', 'Deni'],\n",
    "#         'berat_badan_kg': [70, 80, 90, 100],\n",
    "#         'tinggi_badan_m': [1.65, 1.75, 1.70, 1.80]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Fungsi untuk menghitung BMI\n",
    "# def hitung_bmi(berat_badan, tinggi_badan):\n",
    "#     return berat_badan / (tinggi_badan ** 2)\n",
    "\n",
    "# # Fungsi untuk mengklasifikasikan overweight\n",
    "# def klasifikasikan_overweight(bmi):\n",
    "#     if 30 <= bmi < 35:\n",
    "#         return 'overweight class 1'\n",
    "#     elif 35 <= bmi < 40:\n",
    "#         return 'overweight class 2'\n",
    "#     elif bmi >= 40:\n",
    "#         return 'overweight class 3'\n",
    "#     else:\n",
    "#         return 'normal'\n",
    "\n",
    "# # Membuat kolom baru dengan kategori overweight\n",
    "# df['bmi'] = df.apply(lambda row: hitung_bmi(row['berat_badan_kg'], row['tinggi_badan_m']), axis=1)\n",
    "# df['overweight_category'] = df['bmi'].apply(klasifikasikan_overweight)\n",
    "\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek duplicated data\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump dataset agar tidak mengulang\n",
    "data_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data feature dan target\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data Train dan Test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functiion sizes chek\n",
    "def print_data_sizes(X_train, y_train, X_test, y_test):\n",
    "    print('Train Data')\n",
    "    print('Ukuran X_train : ', X_train.shape)\n",
    "    print('Ukuran y_train : ', y_train.shape)\n",
    "    print('\\n')\n",
    "    print('Test Data')\n",
    "    print('Ukuran X_test : ', X_test.shape)\n",
    "    print('Ukuran y_test : ', y_test.shape)\n",
    "\n",
    "\n",
    "print_data_sizes(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek missing values \n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek missing values \n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek missing values \n",
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek missing values \n",
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handilng Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Numerical Columns into Variable\n",
    "num_cols = X_train.select_dtypes(include=np.number).columns.to_list()\n",
    "\n",
    "# Making data and columns for normal distribution\n",
    "dataNum_normal = []\n",
    "listCols_normal = []\n",
    "\n",
    "# Making data and columns for skewed distribution\n",
    "dataNum_skewed = []\n",
    "listCols_skewed = []\n",
    "\n",
    "# For loop in every numerical column to filer the data distribution into either normal distributed or skewed columns\n",
    "for cols in num_cols:\n",
    "    skewness = X_train[cols].skew()\n",
    "    \n",
    "    # If the data normally distributed\n",
    "    if skewness <= 0.5 and skewness >= -0.5:            \n",
    "        kolom_numerik_normal.append(cols)\n",
    "        dataNum_normal.append([cols, skewness])\n",
    "        \n",
    "    # Elif the data is skewed\n",
    "    elif skewness < -0.5 or skewness > 0.5:\n",
    "        listCols_skewed.append(cols)\n",
    "        dataNum_skewed.append([cols, skewness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Normally distributed columns\n",
    "data_norm = pd.DataFrame(data=dataNum_normal, columns=['kolom_terdistribusi_normal', 'skewness'])\n",
    "data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing skewed columns\n",
    "data_skew = pd.DataFrame(data=dataNum_skewed, columns=['kolom_skewed', 'skewness'])\n",
    "data_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input low variation columns into variable\n",
    "kolom_low_variation = ['surge_multiplier', 'precipIntensity', 'precipProbability', 'uvIndex']\n",
    "\n",
    "# Drop the low variation columns in skewed columns, and add it into normal distribution columns\n",
    "for kolom in kolom_low_variation:\n",
    "    listCols_skewed.remove(kolom) #-> removing low variation columns in skewed columns\n",
    "    listCols_normal.append(kolom) #-> appending low variation columns in normal distributed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping Method for Normal Distribution  \n",
    "winsorizer_normal_dist = Winsorizer(capping_method='gaussian',\n",
    "                            tail='both',\n",
    "                            fold=3,\n",
    "                            variables=listCols_normal,\n",
    "                            missing_values='ignore')\n",
    "\n",
    "# Fit & Transforming X_train \n",
    "X_train_capped = winsorizer_normal_dist.fit_transform(X_train_cleaned)\n",
    "\n",
    "# Transforming X_test\n",
    "X_test_capped = winsorizer_normal_dist.transform(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorizer_skewed = Winsorizer(capping_method='iqr',\n",
    "                            tail='both',\n",
    "                            fold=1.5,\n",
    "                            variables=listCols_skewed,\n",
    "                            missing_values='ignore')\n",
    "\n",
    "# Fit & Transforming X_train \n",
    "X_train_capped = winsorizer_skewed.fit_transform(X_train_cleaned)\n",
    "\n",
    "# Transforming X_test\n",
    "X_test_capped = winsorizer_skewed.transform(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution Comparison\n",
    "def outlier_handling_plot_comparison(df_before, df_after, variable):\n",
    "    \"\"\"\n",
    "    This function is created to plot histograms and boxplots for a variable before and after outlier handling\n",
    "    \n",
    "    Parameters:\n",
    "    - df_before (pandas.DataFrame): DataFrame before outlier handling\n",
    "    - df_after (pandas.DataFrame): DataFrame after outlier handling\n",
    "    - variable (str): The variable to plot\n",
    "    \n",
    "    Example: \n",
    "    num_cols = ['numeric_column']                                   <- Enter the numeric column\n",
    "    for col in num_cols:                                            <- make a for loop in numeric column to access every columns\n",
    "        plot_distribution_comparison(X_train, X_train_capped, col)  <- put X_train, X_train_capped, and columns in the variable\n",
    "\n",
    "    \"\"\"\n",
    "    # Figure Size, and Super Title based on variable\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))                               \n",
    "    fig.suptitle(f'{variable} - Distribution Before and After Outlier Handling')\n",
    "\n",
    "    # Plot Histogram Before\n",
    "    sns.histplot(df_before[variable], bins=30, ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Histogram Before')\n",
    "\n",
    "    # Plot Boxplot Before\n",
    "    sns.boxplot(y=df_before[variable], ax=axes[1, 0], color='lightgreen')\n",
    "    axes[1, 0].set_title('Boxplot Before')\n",
    "\n",
    "    # Plot Histogram After\n",
    "    sns.histplot(df_after[variable], bins=30, ax=axes[0, 1], color='skyblue')\n",
    "    axes[0, 1].set_title('Histogram After')\n",
    "\n",
    "    # Plot Boxplot After\n",
    "    sns.boxplot(y=df_after[variable], ax=axes[1, 1], color='lightgreen')\n",
    "    axes[1, 1].set_title('Boxplot After')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Function\n",
    "num_cols = kolom_numerik \n",
    "for col in num_cols:\n",
    "    outlier_handling_plot_comparison(X_train_cleaned, X_train_capped, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Numerikal dan Kategorikal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column numeric list\n",
    "num_columns = X_train_capped.select_dtypes(include=np.number).columns.tolist()\n",
    "# column categorical list\n",
    "cat_columns = X_train_capped.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print('Numerical List: ', num_columns)\n",
    "print('Catgorical List: ', cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override X_train with spliting list\n",
    "X_train_num = X_train_capped[num_columns]\n",
    "X_train_cat = X_train_capped[cat_columns]\n",
    "# override X_test with spliting list\n",
    "X_test_num = X_test_capped[num_columns]\n",
    "X_test_cat = X_test_capped[cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sizes\n",
    "print_data_sizes(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between categorical columns and 'price' using Kendall Tau's correlation\n",
    "\n",
    "p_values = []\n",
    "interpretation = []\n",
    "cols = []\n",
    "corr = []\n",
    "\n",
    "\n",
    "for col in X_train_cat.columns:\n",
    "  corr_coef, p_value = kendalltau(X_train_cat[col], y_train)\n",
    "\n",
    "  p_values.append(p_value)\n",
    "  cols.append(col)\n",
    "  corr.append(corr_coef)\n",
    "\n",
    "  if p_value < 0.05:\n",
    "    interpretation.append('Significant Correlation')\n",
    "  else :\n",
    "    interpretation.append('No Significant Correlation')\n",
    "\n",
    "cat_corr= pd.DataFrame({'Column Name':cols,\n",
    "              'Correlation Coefficient' : corr,\n",
    "              'P-value':p_values,\n",
    "              'Interpretation': interpretation })\n",
    "\n",
    "cat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter corelation categorical\n",
    "cat_corr[cat_corr['Interpretation'] == \"Significant Correlation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between numerical columns and 'price' using pearsonr and spearmanr correlation\n",
    "\n",
    "p_values = []\n",
    "interpretation = []\n",
    "cols = []\n",
    "corr = []\n",
    "\n",
    "for col in X_train_num.columns:\n",
    "  if abs(X_train_num[col].skew()) < 0.5:                    #For Normally Distributed Columns\n",
    "    corr_coef, p_value = pearsonr(X_train_num[col], y_train)\n",
    "\n",
    "    p_values.append(p_value)\n",
    "    cols.append(col)\n",
    "    corr.append(corr_coef)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "      interpretation.append('Significant Correlation')\n",
    "    else :\n",
    "      interpretation.append('No Significant Correlation')\n",
    "  else:                                                     #For Non Normally Distributed Columns\n",
    "    corr_coef, p_value = spearmanr(X_train_num[col], y_train)\n",
    "\n",
    "    p_values.append(p_value)\n",
    "    cols.append(col)\n",
    "    corr.append(corr_coef)\n",
    "\n",
    "    if p_value < 0.05:\n",
    "      interpretation.append('Significant Correlation')\n",
    "    else :\n",
    "      interpretation.append('No Significant Correlation')\n",
    "\n",
    "num_corr = pd.DataFrame({'Column Name':cols,\n",
    "              'Correlation Coefficient' : corr,\n",
    "              'P-value':p_values,\n",
    "              'Interpretation': interpretation })\n",
    "\n",
    "num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter corelation numerical\n",
    "num_corr[num_corr['Interpretation'] == \"Significant Correlation\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat variable baru Untuk Feature yang berkolerasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat list Feature berkorelasi dari numerikal dan kategorikal\n",
    "selected_col_cat = ['cab_type','name','source','destination']\n",
    "selected_col_num = ['distance','surge_multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append ke x_train dan \n",
    "X_train_cat = X_train_cat[selected_col_cat]\n",
    "X_train_num = X_train_num[selected_col_num]\n",
    "\n",
    "X_test_cat = X_test_cat[selected_col_cat]\n",
    "X_test_num = X_test_num[selected_col_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the categorical data\n",
    "X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the numerical data\n",
    "X_train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling dan Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling menggunakan Standar Scaler karena lebih cocok untuk model linear regression \n",
    "\n",
    "#Initialize the Standar Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit_transform for X_train, transform for X_test\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num) ## only fit in train\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "X_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data agar tidak ngulang\n",
    "dummp1 = X_train_cat.copy()\n",
    "dummp2 = X_test_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan One Hot Encoder karena fitur kategorikal kami tampaknya tidak memiliki hierarki yang ditentukan\n",
    "\n",
    "#Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "\n",
    "#Fit_transform for X_train, transform for X_test\n",
    "X_train_cat_encoded = encoder.fit_transform(X_train_cat)\n",
    "X_test_cat_encoded = encoder.transform(X_test_cat)\n",
    "\n",
    "\n",
    "#Fit_transform for X_train, transform for X_test\n",
    "\n",
    "X_train_cat_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Categorical and Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.concatenate([X_train_num_scaled,X_train_cat_encoded],axis=1)\n",
    "X_test_final = np.concatenate([X_test_num_scaled, X_test_cat_encoded],axis=1)\n",
    "\n",
    "# Transform into dataframe\n",
    "X_train_final = pd.DataFrame(X_train_final)\n",
    "X_test_final = pd.DataFrame(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vi. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi model\n",
    "model_linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vii. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model train\n",
    "model_linreg.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coefficients/slope dan intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coefficients and intercept\n",
    "\n",
    "coefficients = model_linreg.coef_\n",
    "intercept = model_linreg.intercept_\n",
    "\n",
    "params = {\n",
    "    'feature': X_train_final.columns.tolist(),\n",
    "    'coefficient': model_linreg.coef_.tolist()\n",
    "}\n",
    "\n",
    "print('Intercept : ', intercept)\n",
    "pd.DataFrame(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# viii. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluasi Train set\n",
    "y_pred_train = model_linreg.predict(X_train_final)\n",
    "\n",
    "# model evaluasi Test set\n",
    "y_pred_test = model_linreg.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE - Train Set  : ', mean_absolute_error(y_train, y_pred_train))\n",
    "print('MAE - Test Set   : ', mean_absolute_error(y_test, y_pred_test))\n",
    "print('')\n",
    "\n",
    "print('MSE - Train Set  : ', mean_squared_error(y_train, y_pred_train))\n",
    "print('MSE - Test Set   : ', mean_squared_error(y_test, y_pred_test))\n",
    "print('')\n",
    "\n",
    "print('RMSE - Train Set : ', mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('RMSE - Test Set  : ', mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('')\n",
    "\n",
    "print('R2 Score - Train Set : ', r2_score(y_train, y_pred_train))\n",
    "print('R2 Score - Test Set  : ', r2_score(y_test, y_pred_test))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluasi model dengan R2 Score\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred_test, c='blue', label='Data')\n",
    "plt.plot(y_test, y_test, color='red', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'R^2 Score: {r2:.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model dengan asumsi Multivariate Normality\n",
    "\n",
    "# get residual\n",
    "y_test_residuals = y_test - y_pred_test\n",
    "y_test_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot QQ plot residual\n",
    "\n",
    "sm.qqplot(y_test_residuals, line ='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ix. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model saving\n",
    "\n",
    "with open('list_num_cols.txt', 'w') as file_1:\n",
    "  json.dump(selected_col_num, file_1)\n",
    "\n",
    "with open('list_cat_cols.txt', 'w') as file_2:\n",
    "  json.dump(selected_col_cat, file_2)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as file_3:\n",
    "  pickle.dump(scaler, file_3)\n",
    "\n",
    "with open('encoder.pkl', 'wb') as file_4:\n",
    "  pickle.dump(encoder, file_4)\n",
    "\n",
    "with open('model.pkl', 'wb') as file_5:\n",
    "  pickle.dump(model_linreg, file_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and other files\n",
    "\n",
    "with open('list_cat_cols.txt', 'r') as file_1:\n",
    "  list_cat_col = json.load(file_1)\n",
    "\n",
    "with open('list_num_cols.txt', 'r') as file_2:\n",
    "  list_num_col = json.load(file_2)\n",
    "\n",
    "with open(\"model.pkl\", \"rb\") as file_3:\n",
    "  model = pickle.load(file_3)\n",
    "\n",
    "with open(\"scaler.pkl\", \"rb\") as file_4:\n",
    "  scaler = pickle.load(file_4)\n",
    "\n",
    "with open(\"encoder.pkl\", \"rb\") as file_5:\n",
    "  encoder = pickle.load(file_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cara 1\n",
    "# # Drop a random row\n",
    "# random_index = np.random.randint(0, len(df))\n",
    "# inference = df.iloc[random_index]\n",
    "\n",
    "# # Drop the row from the DataFrame\n",
    "# df.drop(index=random_index, inplace=True)\n",
    "\n",
    "# # Save the modified DataFrame to CSV\n",
    "# df.to_csv('modified_dataset.csv', index=False)\n",
    "\n",
    "# # Save the dropped row (inference) to CSV\n",
    "# inference.to_csv('inference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cara 2\n",
    "\n",
    "# # Making a randomized dataset for inference\n",
    "# random_data = [\n",
    "#     {\n",
    "#         'PassengerId': i + 1,\n",
    "#         'Survived': np.random.randint(0, 2),\n",
    "#         'Pclass': np.random.randint(1, 4),\n",
    "#         'Name': 'Name_' + str(i + 1),\n",
    "#         'Sex': np.random.choice(['male', 'female']),\n",
    "#         'Age': np.random.normal(loc=30, scale=10),\n",
    "#         'SibSp': np.random.randint(0, 5),\n",
    "#         'Parch': np.random.randint(0, 3),\n",
    "#         'Ticket': 'Ticket_' + str(i + 1),\n",
    "#         'Fare': np.random.uniform(0, 100),\n",
    "#         'Cabin': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G'], p=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.4]) + str(np.random.randint(1, 100)),\n",
    "#         'Embarked': np.random.choice(['S', 'C', 'Q'])\n",
    "#     }\n",
    "#     for i in range(2)  # Adjust the range according to the desired number of rows\n",
    "# ]\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# random_df = pd.DataFrame(random_data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# random_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Dataset Into Categorical and Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into categorical and numerical columns\n",
    "\n",
    "data_infNum = data_inf[list_num_col]\n",
    "data_infCat = data_inf[list_cat_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features, encoding the categorical features, and concatenating them back again\n",
    "\n",
    "data_inf_scaled_num = scaler.transform(data_infNum)\n",
    "data_inf_cat_encoded = encoder.transform(data_infCat)\n",
    "data_inference_final = np.concatenate([data_inf_scaled_num, data_inf_cat_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "\n",
    "y_pred_inference = model.predict(data_inference_final)\n",
    "y_pred_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xi. Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
